{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script of DeepAnno16, including: \n",
    "1. dataset construction\n",
    "2. training procedures\n",
    "3. comparison with V-xtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182357/2645582735.py:21: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon, entropy, ranksums, norm, mannwhitneyu\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "from collections import Counter\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "ori_data_dir = 'data/SILVA_data'\n",
    "crossVal_data_dir = 'data/Ten_CrossValidation'\n",
    "plt.rcParams[\"font.family\"] = \"Arial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import matplotlib\n",
    "shutil.rmtree(matplotlib.get_cachedir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training data set is divided according to 8:1:1 ratio, and each of the ten data sets is used as a test set for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SILVA dataset preprocessing\n",
    "SILVA_metainfo = pd.read_csv(os.path.join(ori_data_dir, 'silva_metainfo.csv'),)\n",
    "SILVA_seqs = pd.read_csv(os.path.join(ori_data_dir, 'silva_seqs_id_only.csv'),)\n",
    "print(f'SILVA dataset contains {SILVA_metainfo.shape, SILVA_seqs.shape} seqs.')\n",
    "\n",
    "SILVA_metainfo_clear = SILVA_metainfo.copy()\n",
    "SILVA_metainfo_clear['Genus'] = SILVA_metainfo_clear['Genus'].apply(lambda x: x.lower())\n",
    "SILVA_metainfo_clear['Species'] = SILVA_metainfo_clear['Species'].apply(lambda x: x.lower())\n",
    "# remove plasmid and phage seqs\n",
    "SILVA_metainfo_clear = SILVA_metainfo_clear[SILVA_metainfo_clear['Species'].apply(lambda x: ('plasmid' not in x) and ('phage' not in x))]\n",
    "SILVA_metainfo_clear = SILVA_metainfo_clear[SILVA_metainfo_clear['Genus'].apply(lambda x: ('plasmid' not in x) and ('phage' not in x))]\n",
    "SILVA_metainfo_clear.reset_index(inplace=True, drop=True)\n",
    "\n",
    "SILVA_metainfo = SILVA_metainfo[SILVA_metainfo['silva_id'].isin(list(SILVA_metainfo_clear['silva_id']))]\n",
    "SILVA_metainfo.reset_index(inplace=True, drop=True)\n",
    "SILVA_metainfo.to_csv(os.path.join(ori_data_dir, 'silva_metainfo_clear.csv'), index=False)\n",
    "print(f'SILVA dataset contains {SILVA_metainfo.shape} bacterial seqs.')\n",
    "\n",
    "SILVA_metainfo['Genus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239175, 11)\n",
      "Loss because of v1 is 0\n",
      "Loss because of v2 is 8\n",
      "Loss because of v3 is 8\n",
      "Loss because of v4 is 5\n",
      "Loss because of v5 is 7\n",
      "Loss because of v6 is 6\n",
      "Loss because of v7 is 24\n",
      "Loss because of v8 is 57\n",
      "Loss because of sum is 0\n",
      "(238890, 12) seqs with complete 9 V-regions annotation.\n",
      "(192498, 12) seqs with incomplete 9 V-regions annotation.\n"
     ]
    }
   ],
   "source": [
    "## Get the golden standard of V-regions\n",
    "from HVR_info_split_clear import *\n",
    "def str_list(str = '\"[1204, 1224]\"'):\n",
    "    str = str.strip('\"[').strip(']\"')\n",
    "    str = str.split(', ')\n",
    "    str = [int(x) for x in str]\n",
    "    return str\n",
    "\n",
    "def str_lens(x):\n",
    "    x = str_list(x)\n",
    "    return x[1] - x[0]\n",
    "\n",
    "\n",
    "HVR_info_merged = ori_data_dir + '/HVRs_info' + '_merged.csv'\n",
    "HVR_info_split_main(HVR_info_merged)\n",
    "\n",
    "# further remove those aligned in a wrong way\n",
    "ori_df = os.path.join(ori_data_dir, 'HVRs_info_merged_complete.csv')\n",
    "ori_df = pd.read_csv(ori_df)\n",
    "ori_df = pd.DataFrame(ori_df, columns=([f'v{i+1}' for i in range(9)] + ['lens', 'id']))\n",
    "print(ori_df.shape)\n",
    "\n",
    "for col in ori_df.columns[: -2]:\n",
    "    # seqs of each V-region should meet the length constraint\n",
    "    ori_df[col] = ori_df[col].apply(str_lens)\n",
    "    t_start = ori_df.shape[0]\n",
    "\n",
    "    if col == 'v9':\n",
    "        key_v9 = ori_df[ori_df['v9'] <= 0]\n",
    "        continue\n",
    "    ori_df = ori_df[ori_df[col] > 0]\n",
    "    if col != 'v1' and col != 'v9':\n",
    "        ori_df = ori_df[ori_df[col] < 400]\n",
    "    ori_df.reset_index(inplace=True, drop=True)\n",
    "    print(f'Loss because of {col} is {t_start - ori_df.shape[0]}')\n",
    "\n",
    "t_start = ori_df.shape[0]\n",
    "ori_df = ori_df[(ori_df['v1'] + ori_df['v2'] + ori_df['v3'] + ori_df['v4'] + ori_df['v5'] + ori_df['v6'] + ori_df['v7'] + ori_df['v8'] + ori_df['v9'])  < ori_df['lens']]\n",
    "ori_df.reset_index(inplace=True, drop=True)\n",
    "print(f'Loss because of sum is {t_start - ori_df.shape[0]}')\n",
    "clear_df = ori_df.copy()\n",
    "\n",
    "ori_df = os.path.join(ori_data_dir, 'HVRs_info_merged_complete.csv')\n",
    "ori_df = pd.read_csv(ori_df)\n",
    "\n",
    "# remove seqs of plasmid and phage\n",
    "ori_df = ori_df[ori_df['id'].isin(list(clear_df['id'])) & ori_df['id'].isin(list(SILVA_metainfo['silva_id_wrong']))]\n",
    "print(f'{ori_df.shape} seqs with complete 9 V-regions annotation.')\n",
    "ori_df.to_csv(os.path.join(ori_data_dir, 'HVRs_info_merged_complete_clear.csv'), index=False)\n",
    "\n",
    "# replace incomplete seqs\n",
    "ori_com_df = pd.read_csv(os.path.join(ori_data_dir, 'HVRs_info_merged_complete.csv'))\n",
    "ori_incom_df = pd.read_csv(os.path.join(ori_data_dir, 'HVRs_info_merged_incomplete.csv'))\n",
    "ori_com_incom_df = pd.concat([ori_com_df, ori_incom_df])\n",
    "ori_com_incom_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ori_incom_df = ori_com_incom_df[(~ori_com_incom_df['id'].isin(list(ori_df['id']))) & ori_com_incom_df['id'].isin(list(SILVA_metainfo['silva_id_wrong']))]\n",
    "ori_incom_df.reset_index(inplace=True, drop=True)\n",
    "ori_incom_df.to_csv(os.path.join(ori_data_dir, 'HVRs_info_merged_incomplete_clear.csv'), index=False)\n",
    "print(f'{ori_incom_df.shape} seqs with incomplete 9 V-regions annotation.')\n",
    "\n",
    "# remove temperory files\n",
    "os.remove(os.path.join(ori_data_dir, 'HVRs_info_merged_complete.csv'))\n",
    "os.remove(os.path.join(ori_data_dir, 'HVRs_info_merged_incomplete.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the training and testing set have (238890, 12) samples.\n"
     ]
    }
   ],
   "source": [
    "## 10-fold cross validation dataset\n",
    "SILVA_metainfo_idRefine = pd.DataFrame(SILVA_metainfo, columns=['silva_id_wrong','silva_id'])\n",
    "clear_ori_with_9hvrs = os.path.join(ori_data_dir, 'HVRs_info_merged_complete_clear.csv')\n",
    "clear_ori_with_9hvrs = pd.read_csv(clear_ori_with_9hvrs)\n",
    "\n",
    "# refine the silva id\n",
    "clear_ori_with_9hvrs_clearPhagePlasmid = clear_ori_with_9hvrs.copy()\n",
    "clear_ori_with_9hvrs_clearPhagePlasmid = pd.merge(clear_ori_with_9hvrs_clearPhagePlasmid, SILVA_metainfo_idRefine, how='left', left_on='id', right_on='silva_id_wrong')\n",
    "clear_ori_with_9hvrs_clearPhagePlasmid['id'] = clear_ori_with_9hvrs_clearPhagePlasmid['silva_id']\n",
    "clear_ori_with_9hvrs_clearPhagePlasmid.drop([\"silva_id\", \"silva_id_wrong\"], axis=1, inplace=True)\n",
    "clear_ori_with_9hvrs_clearPhagePlasmid.to_csv(os.path.join(crossVal_data_dir, 'HVRs_info_merged_complete_clearBac.csv'), index=False)\n",
    "\n",
    "print(f'All the training and testing set have {clear_ori_with_9hvrs_clearPhagePlasmid.shape} samples.')\n",
    "\n",
    "# split training and testing set\n",
    "train_test_ratio = 0.1\n",
    "test_n = int(clear_ori_with_9hvrs_clearPhagePlasmid.shape[0] * train_test_ratio)\n",
    "data_n = clear_ori_with_9hvrs_clearPhagePlasmid.shape[0]\n",
    "\n",
    "df = clear_ori_with_9hvrs_clearPhagePlasmid.sample(frac=1, random_state=9931).reset_index(drop=True)\n",
    "for i in range(10):\n",
    "    start_index = i * test_n\n",
    "    end_index = (i + 1) * test_n if i < 9 else data_n\n",
    "    \n",
    "    test_df = df.iloc[start_index:end_index]\n",
    "    train_df = pd.concat([df.iloc[0:start_index], df.iloc[end_index:data_n]])\n",
    "\n",
    "    train_df.to_csv(os.path.join(crossVal_data_dir, f\"trainingSet_{i}Fold.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(crossVal_data_dir, f\"testingSet_{i}Fold.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ten-fold cross validation\n",
    "# config file\n",
    "ori_json = '16sDeepSeg/config_train.json'\n",
    "with open(ori_json, \"r\") as file:\n",
    "    ori_json = json.load(file)\n",
    "    \n",
    "def create_train_config(ori_json, data_dir = '', save_dir = ''):\n",
    "    json_config = ori_json.copy()\n",
    "    json_config['data_loader']['args']['data_dir'] = data_dir\n",
    "    json_config['trainer']['save_dir'] = save_dir\n",
    "    # 修改log config的位置\n",
    "    json_config['log_config'] = '16sDeepSeg/logger/logger_config.json'\n",
    "    return json_config\n",
    "\n",
    "with open( f'run_ten_Fold.sh', 'w') as f:\n",
    "    for fold_i in range(10):\n",
    "        fold_dir = os.path.join(crossVal_data_dir, f'Fold_{fold_i}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        train_data = os.path.join(crossVal_data_dir, f\"trainingSet_{fold_i}Fold.csv\")\n",
    "        save_path = os.path.join(fold_dir, f\"trainingSet_{fold_i}Fold\")\n",
    "        config_i = create_train_config(ori_json, data_dir = train_data, save_dir = save_path)\n",
    "        \n",
    "        # 保存相应的json文件\n",
    "        out_json_path = os.path.join(fold_dir, f\"config_train_{fold_i}Fold.json\")\n",
    "        with open(out_json_path, \"w\") as file:\n",
    "            json.dump(config_i, file, indent=4)\n",
    "        \n",
    "        # 生成跑模型的命令\n",
    "        model_train_py = '/data1/hyzhang/Projects/EcoPrimer_git/DeepEcoPrimer/16sDeepSeg/train.py' # 用绝对路径\n",
    "        run_cmd = f'python {model_train_py} --config {out_json_path}'\n",
    "\n",
    "        f.write(f'{run_cmd}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用于画图的函数\n",
    "def plot_auc_pr(y_true, y_pred, title=\"\", fig_name = './AUC.jpg'):\n",
    "    # roc auc\n",
    "    auc_scores = []\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    label_names = ['Conserved'] + [f'V{i+1}' for i in range(9)]\n",
    "    for i in range(10):\n",
    "        y_label = np.array(y_true == i, dtype=np.int8)\n",
    "        y_pred_ = np.array(y_pred[:, i])   \n",
    "        fpr = dict()\n",
    "        tpr = dict() \n",
    "        roc_auc = dict()\n",
    "        # calculate the auc\n",
    "        auc_score = metrics.roc_auc_score(y_label, y_pred_)\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        # calculate the ROC curve\n",
    "        fpr[0], tpr[0], _ = metrics.roc_curve(y_label, y_pred_)\n",
    "        roc_auc[0] = metrics.auc(fpr[0], tpr[0])\n",
    "        plt.plot(fpr[0], tpr[0],\n",
    "            lw=lw, label= label_names[i] + ' (AUC = %0.3f)' % roc_auc[0]) # 16sDeepSeg Position-wise\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    fontsize = 14\n",
    "    plt.xlabel('False Positive Rate', fontsize = fontsize)\n",
    "    plt.ylabel('True Positive Rate', fontsize = fontsize)\n",
    "    #plt.title('Receiver Operating Characteristic Curve', fontsize = fontsize)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_name , dpi = 600)\n",
    "    \n",
    "    # pr auc\n",
    "    plt.figure()\n",
    "    plt.plot([0,1], [1, 0], linestyle='--')\n",
    "    for i in range(10):\n",
    "        y_label = np.array(y_true == i, dtype=np.int8)\n",
    "        y_pred_ = np.array(y_pred[:, i]) \n",
    "        lr_precision, lr_recall, _ = metrics.precision_recall_curve(y_label, y_pred_)    \n",
    "        plt.plot(lr_recall, lr_precision, lw = 2, label= label_names[i] + ' (AP = %0.3f)' % metrics.average_precision_score(y_label, y_pred_))\n",
    "    fontsize = 14\n",
    "    plt.xlabel('Recall', fontsize = fontsize)\n",
    "    plt.ylabel('Precision', fontsize = fontsize)\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_name.replace('ROC', 'PR') , dpi = 600)\n",
    "    \n",
    "    return(auc_scores)\n",
    "\n",
    "def plot_recall_precision(\n",
    "    cm,\n",
    "    target_names,\n",
    "    fig_path=\"\",\n",
    "    auc_scores = [],\n",
    "):\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1_scores = []\n",
    "    for i in range(10):\n",
    "        recall = cm[i, i] / cm[i, :].sum()\n",
    "        precision = cm[i, i] / cm[:, i].sum()\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # 计算f1 score\n",
    "        f1_score = 2 * (recall * precision) / (recall + precision)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    plt.figure(figsize=(4, 4)) # A4纸的一半\n",
    "    plt.rcParams['axes.labelsize'] = 12 # 以磅为单位\n",
    "    barWidth=0.19\n",
    "    metrics_to_plot = {'ROC_auc': auc_scores, 'Recall': recalls, 'Precision': precisions, 'F1_score': f1_scores}\n",
    "    # plot the bar chart\n",
    "    for met_i, (met_nm, met_val) in enumerate(metrics_to_plot.items()):\n",
    "        plt.bar([x+met_i*barWidth for x in range(10)], height=met_val, label=met_nm, width=barWidth,)\n",
    "    # plot the grid and x,y ticks\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.xticks([x+3*barWidth/2 for x in range(10)], target_names, rotation=315)\n",
    "    def percentage_formatter(x, pos):\n",
    "        return f'{x*100:.4f}%'\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(percentage_formatter))\n",
    "    plt.ylim([0.999, 1.0001])\n",
    "    # add legend\n",
    "    plt.legend(bbox_to_anchor=(1.001, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi = 600)\n",
    "\n",
    "def plot_conf(\n",
    "    cm,\n",
    "    target_names,\n",
    "    cmap=\"Blues\",\n",
    "    normalize=True,\n",
    "    fig_path=\"\",\n",
    "    auc_scores = [],\n",
    "):\n",
    "    # plot the recall and precision fig\n",
    "    plot_recall_precision(cm, target_names, fig_path.replace('confusion_matrix', 'recall_precision'), auc_scores = auc_scores)\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=315)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:0.2f}%\".format(cm[i, j] * 100),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:,}\".format(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\", size=15)\n",
    "    plt.xlabel(\"Predicted label\", size=15)\n",
    "    plt.savefig(fig_path, format=\"svg\", bbox_inches=\"tight\", dpi=600)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试16sDeepSeg模型base-level的性能\n",
    "fig_dir = os.path.join(crossVal_data_dir, 'Saved_figs')\n",
    "\n",
    "# 首先跑Vxtrator的结果\n",
    "# !python run_Vxtractor.py\n",
    "\n",
    "# 跑各个16sDeepSeg模型的结果\n",
    "ori_json_test = '16sDeepSeg/config_test.json'\n",
    "with open(ori_json_test, \"r\") as file:\n",
    "    ori_json_test = json.load(file)\n",
    "    \n",
    "def create_test_config(ori_json, data_dir = '', save_dir = ''):\n",
    "    json_config = ori_json.copy()\n",
    "    json_config['data_loader']['args']['data_dir'] = data_dir\n",
    "    json_config['trainer']['save_dir'] = save_dir\n",
    "    # 修改log config的位置\n",
    "    json_config['log_config'] = '16sDeepSeg/logger/logger_config.json'\n",
    "    return json_config\n",
    "\n",
    "def find_best_model(save_dir = 'data/Ten_CrossValidation/Fold_6/trainingSet_6Fold/models/16sRNA_seg_Unet/'):\n",
    "    model_path = os.listdir(save_dir)[0]\n",
    "    model_path = os.path.join(save_dir, model_path)\n",
    "    model_best_path = os.path.join(model_path, 'model_best.pth')\n",
    "    return model_best_path\n",
    "\n",
    "with open( f'run_ten_Fold_test.sh', 'w') as f:\n",
    "    for fold_i in range(10):\n",
    "        fold_dir = os.path.join(crossVal_data_dir, f'Fold_{fold_i}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        train_data = os.path.join(crossVal_data_dir, f\"testingSet_{fold_i}Fold.csv\")\n",
    "        save_path = os.path.join(fold_dir, f\"testingSet_{fold_i}Fold\")\n",
    "        config_i = create_test_config(ori_json_test, data_dir = train_data, save_dir = save_path)\n",
    "        \n",
    "        # 保存相应的json文件\n",
    "        out_json_path = os.path.join(fold_dir, f\"config_test_{fold_i}Fold.json\")\n",
    "        with open(out_json_path, \"w\") as file:\n",
    "            json.dump(config_i, file, indent=4)\n",
    "        \n",
    "        # 生成跑模型的命令\n",
    "        model_train_py = '16sRedSeg/module_output.py' # 用绝对路径\n",
    "        try:\n",
    "            model_best_path = find_best_model(save_dir = os.path.join(fold_dir, f\"trainingSet_{fold_i}Fold/models/16sRNA_seg_Unet/\"))\n",
    "        except:\n",
    "            model_best_path = '----'\n",
    "            \n",
    "        output_csv = os.path.join(fold_dir, f\"testing_set_segmentation_16sDeepSeg.csv\")\n",
    "        run_cmd = f'python {model_train_py} -c {out_json_path} -r {model_best_path} --input {train_data} --output {output_csv} --plot_auc ./confusion_matrix.svg'\n",
    "\n",
    "        f.write(f'{run_cmd}\\n') \n",
    "        \n",
    "# 首先做supplementary fig 2\n",
    "fold_dirs = [f'data/Ten_CrossValidation/Fold_{i}' for i in range(10)]\n",
    "position_wise_true, position_wise_pred = [], []\n",
    "for fold_dir in fold_dirs:\n",
    "    try:\n",
    "        position_wise_true_t, position_wise_pred_t = np.load(os.path.join(fold_dir, 'position_wise_true.npy'),), np.load(os.path.join(fold_dir, 'position_wise_pred.npy'),)\n",
    "    except:\n",
    "        continue\n",
    "    position_wise_true.append(position_wise_true_t)\n",
    "    position_wise_pred.append(position_wise_pred_t)\n",
    "position_wise_true, position_wise_pred = np.concatenate(position_wise_true, axis=0), np.concatenate(position_wise_pred, axis=0)\n",
    "conf_mat = confusion_matrix(y_true=position_wise_true, y_pred=np.argmax(position_wise_pred, axis=1))\n",
    "conf_fig_path = os.path.join(fig_dir, 'confusion_matrix.svg')\n",
    "\n",
    "auc_scores = plot_auc_pr(y_pred=position_wise_pred, y_true=position_wise_true, fig_name=conf_fig_path.replace('confusion_matrix', 'ROC'))\n",
    "plot_conf(conf_mat, target_names = ['Conserved'] + [f'V{i+1}' for i in range(9)], fig_path=conf_fig_path, normalize=True, auc_scores=auc_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用于计算IOU和划分精度的函数\n",
    "# plot_conf(conf_mat, target_names = ['Conserved'] + [f'V{i+1}' for i in range(9)], fig_path=conf_fig_path, normalize=True, auc_scores=auc_scores)\n",
    "def compute_iou(rec1, rec2):\n",
    "    \"\"\"\n",
    "    computing IOU\n",
    "    :param rec1: (y0, y1), which reflects (left, right)\n",
    "    :param rec2: (y0, y1)\n",
    "    :return: scala value of IOU\n",
    "    \"\"\"\n",
    "    # computing area of each rectangles\n",
    "    S_rec1 = (rec1[1] - rec1[0])\n",
    "    S_rec2 = (rec2[1] - rec2[0])\n",
    "    # computing the sum_area\n",
    "    sum_area = S_rec1 + S_rec2\n",
    "    # find the each edge of intersect rectangle\n",
    "    left_line = max(rec1[0], rec2[0])\n",
    "    right_line = min(rec1[1], rec2[1])\n",
    "    # judge if there is an intersect\n",
    "    if left_line >= right_line:\n",
    "        return 0\n",
    "    else:\n",
    "        intersect = (right_line - left_line)\n",
    "        return (intersect / (sum_area - intersect)) * 1.0\n",
    "\n",
    "def str_list(str = '\"[1204, 1224]\"'):\n",
    "    if 'notfound' in str or 'wrongorder' in str:\n",
    "        # wrong pred from vxtractor\n",
    "        return [0, 0]\n",
    "    # pred from vxtractor\n",
    "    if '[' not in str:\n",
    "        str = str.split()[0]\n",
    "        str = str.strip(\"'\")\n",
    "        str = str.split('-')\n",
    "    # pred from 16sDeepSeg\n",
    "    else:\n",
    "        str = str.strip('\"[').strip(']\"')\n",
    "        str = str.split(', ')\n",
    "        \n",
    "    str = [int(float(x)) for x in str]\n",
    "    return str\n",
    "\n",
    "def cal_iou_list(real, pred, log_file, acc_cut_off = 0.3):\n",
    "    iou_score = []\n",
    "    total_seq = len(real)\n",
    "    none_seq = 0\n",
    "    for reg1, reg2 in zip(real, pred):\n",
    "        reg1 = str_list(reg1)\n",
    "        reg2 = str_list(reg2)\n",
    "        iou_t = compute_iou(reg1, reg2)\n",
    "        if iou_t < acc_cut_off:\n",
    "            none_seq += 1\n",
    "        iou_score.append(iou_t)\n",
    "    # print the result to log file\n",
    "    print(f'Average iou: {np.mean(iou_score)}', file=log_file)\n",
    "    print(f'Inaccurately annotated seqs: {none_seq / total_seq}', file=log_file) # iou小于cut_off的序列比例\n",
    "    res_df = pd.DataFrame(iou_score)\n",
    "    recall_t = 1 - none_seq / total_seq\n",
    "    return res_df, np.mean(iou_score), recall_t\n",
    "    \n",
    "def cal_iou_df(real_df, pred_df, primer = 'p2', log_file = None, acc_cut_off = 0.5):\n",
    "    query_df_rna = pd.merge(real_df, pred_df, on='id', how='inner')\n",
    "    query_df_rna.reset_index(inplace=True, drop=True)\n",
    "    # get the list to calculate the iou score\n",
    "    pred_primer_regions = list(query_df_rna[primer])\n",
    "    label_primer_regions = list(query_df_rna[primer + '_label'])\n",
    "    \n",
    "    res_df, iou_mean, recall_t = cal_iou_list(label_primer_regions, pred_primer_regions, log_file, acc_cut_off = acc_cut_off)\n",
    "    # res_df.to_csv(f'{primer}.csv', index = False)\n",
    "    return iou_mean, recall_t\n",
    "\n",
    "# the func to do one fold testing\n",
    "def test_one_fold(folds_root = crossVal_data_dir, fold_num = 0, acc_cut_off = 0.5):\n",
    "    fold_dir = os.path.join(folds_root, f'Fold_{fold_num}')\n",
    "    df_evaluation = []\n",
    "    \n",
    "    for model_checked_nm in ['16sDeepSeg', 'Vxtractor']:\n",
    "        # check 16sDeepSeg & Vxtractor model\n",
    "        log_file = open(f'{fold_dir}/log_{model_checked_nm}.txt', 'w')\n",
    "        goldenLabel_df = pd.read_csv(f'{folds_root}/testingSet_{fold_num}Fold.csv')\n",
    "        if model_checked_nm == 'Vxtractor':\n",
    "            pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
    "            cols_obj = ['Sequence'] + [f'V{i+1}' for i in range(9)]\n",
    "            pred_df = pd.DataFrame(pred_df, columns=cols_obj)\n",
    "            pred_df.rename({'Sequence': 'id'}, axis=1, inplace=True)\n",
    "            pred_df['id'] = pred_df['id'].apply(lambda x: x.strip(\"'\"))\n",
    "            for i in range(9):\n",
    "                pred_df.rename({f'V{i+1}': f'v{i+1}'}, axis=1, inplace=True)\n",
    "        else:\n",
    "            pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv')\n",
    "        \n",
    "        # get running time\n",
    "        if model_checked_nm == 'Vxtractor':\n",
    "            run_time = pd.read_csv('data/Ten_CrossValidation/Vxtractor_10Fold.csv')\n",
    "            run_time_foldx = run_time.iloc[fold_num, 1]\n",
    "        else:\n",
    "            with open(f'{fold_dir}/16sDeepSeg_running_time.txt', 'r') as f:\n",
    "                run_time_foldx = float(f.readline().strip())\n",
    "        \n",
    "        # calculate model efficacy\n",
    "        pred_with_nineVregion_df = pred_df.copy()\n",
    "        for i in range(9):\n",
    "            if model_checked_nm == 'Vxtractor':\n",
    "                pred_with_nineVregion_df = pred_with_nineVregion_df[pred_with_nineVregion_df[f'v{i+1}'].apply(lambda x: ('notfound' not in x) and ('wrongorder' not in x))]\n",
    "            else:\n",
    "                pred_with_nineVregion_df = pred_with_nineVregion_df[pred_with_nineVregion_df[f'v{i+1}'].apply(lambda x: ('-1' not in x))]\n",
    "            pred_with_nineVregion_df.reset_index(inplace=True, drop=True)\n",
    "        number_pred_with_nineVregion = pred_with_nineVregion_df.shape[0]\n",
    "        print(f'Number of seqs with 9 V-regions: {number_pred_with_nineVregion}', file=log_file)\n",
    "        \n",
    "        # calculate the iou score\n",
    "        print('Start iou cal.', file=log_file)\n",
    "        iou_total = 0.0\n",
    "        # calculate 9 v-regions\n",
    "        df_t = {'Fold': fold_num, 'Methods': model_checked_nm, 'Running_time': run_time_foldx, 'Support_num': pred_df.shape[0], 'Efficacy_num': number_pred_with_nineVregion}\n",
    "        for p_i in range(9):\n",
    "            col = f'v{p_i+1}'\n",
    "            with_primer_aligned = goldenLabel_df[~goldenLabel_df[col].isin([\"[-1, -1]\"])]\n",
    "            print(f'Region {col}: {with_primer_aligned.shape} seqs with golden annotations.', file=log_file)\n",
    "            with_primer_aligned.reset_index(inplace=True, drop=True)\n",
    "            with_primer_aligned = pd.DataFrame(with_primer_aligned, columns=['id', col])\n",
    "            with_primer_aligned.rename(columns = {col: col + '_label'}, inplace = True)\n",
    "            # calculate iou score\n",
    "            iou_mean, recall_t = cal_iou_df(with_primer_aligned, pred_df = pred_df, primer=col, log_file = log_file, acc_cut_off = acc_cut_off)\n",
    "            iou_total += iou_mean\n",
    "            df_t[col + '_iou_score'] = iou_mean\n",
    "            df_t[col + '_recall'] = recall_t\n",
    "        \n",
    "        # calculate the mean iou score\n",
    "        iou_total /= 9\n",
    "        print(f'Total mean iou score is {iou_total}', file=log_file)\n",
    "        log_file.close()\n",
    "        df_evaluation.append(df_t)\n",
    "    # df_evaluation = pd.DataFrame(df_evaluation)\n",
    "    return df_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
      "/tmp/ipykernel_98625/1219654103.py:82: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n"
     ]
    }
   ],
   "source": [
    "## 比较16sDeepSeg模型和V-xtractor的划分性能\n",
    "\n",
    "# 生成table 1的结果\n",
    "acc_cutoff = 0.5\n",
    "df_16sDeepSeg_Vxtractor_eva = []\n",
    "for foid_i in range(10):\n",
    "    df_16sDeepSeg_Vxtractor_eva_foldx = test_one_fold(\n",
    "        folds_root=crossVal_data_dir,\n",
    "        fold_num=foid_i,\n",
    "        acc_cut_off=acc_cutoff,\n",
    "    )\n",
    "    df_16sDeepSeg_Vxtractor_eva += df_16sDeepSeg_Vxtractor_eva_foldx\n",
    "df_16sDeepSeg_Vxtractor_eva = pd.DataFrame(df_16sDeepSeg_Vxtractor_eva)\n",
    "df_16sDeepSeg_Vxtractor_eva.to_csv(os.path.join(crossVal_data_dir, f'TenFoldCrossValidation_cutoff_{acc_cutoff}.csv'), index=False)\n",
    "\n",
    "# save均值和方差信息\n",
    "df_16sDeepSeg_Vxtractor_eva['Efficacy'] = df_16sDeepSeg_Vxtractor_eva['Efficacy_num'] / df_16sDeepSeg_Vxtractor_eva['Support_num']\n",
    "writer = pd.ExcelWriter(f'data/Ten_CrossValidation/Saved_figs/TenFoldCrossValidation_cutoff_{acc_cutoff}.xlsx', engine=\"xlsxwriter\")\n",
    "df_16sDeepSeg_Vxtractor_eva.groupby('Methods').mean().to_excel(writer, sheet_name=\"Mean\", )\n",
    "df_16sDeepSeg_Vxtractor_eva.groupby('Methods').std().to_excel(writer, sheet_name=\"Std\", )\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hyzhang/mambaforge/envs/EcoPrimer/lib/python3.8/site-packages/scipy/stats/_morestats.py:3337: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/data1/hyzhang/mambaforge/envs/EcoPrimer/lib/python3.8/site-packages/scipy/stats/_morestats.py:3337: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/data1/hyzhang/mambaforge/envs/EcoPrimer/lib/python3.8/site-packages/scipy/stats/_morestats.py:3351: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# 添加pvalue等信息\n",
    "df_16sDeepSeg_Vxtractor_eva_wilcox = df_16sDeepSeg_Vxtractor_eva.groupby('Methods').agg(lambda x: list(x))\n",
    "df_16sDeepSeg_Vxtractor_eva_wilcox = df_16sDeepSeg_Vxtractor_eva_wilcox.transpose()\n",
    "\n",
    "for test_ind in df_16sDeepSeg_Vxtractor_eva_wilcox.index:\n",
    "    RedSeg_ls, Vxtractor_ls = df_16sDeepSeg_Vxtractor_eva_wilcox.loc[test_ind, '16sDeepSeg'], df_16sDeepSeg_Vxtractor_eva_wilcox.loc[test_ind, 'Vxtractor']\n",
    "    # 执行Wilcoxon秩和检验\n",
    "    try:\n",
    "        statistic, p_value = wilcoxon(RedSeg_ls, Vxtractor_ls)\n",
    "    except:\n",
    "        statistic, p_value = -1, -1\n",
    "    df_16sDeepSeg_Vxtractor_eva_wilcox.loc[test_ind, 'wilcox_pval'] = p_value\n",
    "\n",
    "df_16sDeepSeg_Vxtractor_eva_wilcox.to_csv(f'data/Ten_CrossValidation/Saved_figs/TenFoldCrossValidation_wilcoxPval_cutoff_{acc_cutoff}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 重新换其他metrics查看Vxtractor和RedSeg划分结果：\n",
    "* 先看看保守区长度情况(已经确定较长，试试看vx的结果是不是都覆盖了RedSeg结果\n",
    ")\n",
    "* 划分的保守区使用MSA看看具体保守性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion_cutoff = 0.1\n",
    "\n",
    "def check_intersection(rec1, rec2):\n",
    "    \"\"\"\n",
    "    计算一下是否Vxtractor更长的片段包含了RedSeg的短片段\n",
    "    \"\"\"\n",
    "    # computing area of each rectangles\n",
    "    S_rec1 = (rec1[1] - rec1[0])\n",
    "    S_rec2 = (rec2[1] - rec2[0])\n",
    "    # find the each edge of intersect rectangle\n",
    "    left_line = max(rec1[0], rec2[0])\n",
    "    right_line = min(rec1[1], rec2[1])\n",
    "    if -1 in rec1 or -1 in rec2:\n",
    "        return -1\n",
    "    # judge if there is an intersect\n",
    "    if left_line >= right_line:\n",
    "        return 0\n",
    "    else:\n",
    "        intersect = (right_line - left_line)\n",
    "        return (intersect / min(S_rec1, S_rec2)) * 1.0\n",
    "    \n",
    "def test_one_fold_intersection(folds_root = crossVal_data_dir, fold_num = 0,):\n",
    "    # 20240425：重新计算一下两个模型的metrics\n",
    "    fold_dir = os.path.join(folds_root, f'Fold_{fold_num}')\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    fold_dir_conserved = os.path.join(fold_dir, f'Conserved')\n",
    "    os.makedirs(fold_dir_conserved, exist_ok=True)\n",
    "    \n",
    "    model_preds = []\n",
    "    for model_checked_nm in ['16sDeepSeg', 'Vxtractor']:\n",
    "        # check 16sDeepSeg & Vxtractor model\n",
    "        if model_checked_nm == 'Vxtractor':\n",
    "            pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
    "            cols_obj = ['Sequence'] + [f'V{i+1}' for i in range(9)]\n",
    "            pred_df = pd.DataFrame(pred_df, columns=cols_obj)\n",
    "            pred_df.rename({'Sequence': 'id'}, axis=1, inplace=True)\n",
    "            pred_df['id'] = pred_df['id'].apply(lambda x: x.strip(\"'\"))\n",
    "            for i in range(9):\n",
    "                pred_df.rename({f'V{i+1}': f'v{i+1}'}, axis=1, inplace=True)\n",
    "                pred_df[f'v{i+1}'] = pred_df[f'v{i+1}'].apply(lambda x: [int(i.split(' HMM=')[0]) for i in x.strip(\"'\").split('-')] if (('notfound' not in x) and ('wrongorder' not in x)) else [-1, -1])\n",
    "        else:\n",
    "            pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv')\n",
    "            for i in range(9):\n",
    "                pred_df[f'v{i+1}'] = pred_df[f'v{i+1}'].apply(lambda x: eval(x))\n",
    "        \n",
    "        # 获取conserved区域信息\n",
    "        for conserve_i in range(8):\n",
    "            pred_df[f'conserve_{conserve_i+2}_{model_checked_nm}'] = pred_df.apply(lambda x: [x[f'v{conserve_i+1}'][-1], x[f'v{conserve_i+2}'][0]] , axis = 1)\n",
    "        model_preds.append(pred_df.loc[:, ['id'] + [f'conserve_{i}_{model_checked_nm}' for i in range(2, 10)]])\n",
    "    \n",
    "    model_preds = pd.merge(model_preds[0], model_preds[1], on='id', how='inner')\n",
    "    \n",
    "    return model_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16075/2007880753.py:29: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conserve_2_16sDeepSeg</th>\n",
       "      <th>conserve_3_16sDeepSeg</th>\n",
       "      <th>conserve_4_16sDeepSeg</th>\n",
       "      <th>conserve_5_16sDeepSeg</th>\n",
       "      <th>conserve_6_16sDeepSeg</th>\n",
       "      <th>conserve_7_16sDeepSeg</th>\n",
       "      <th>conserve_8_16sDeepSeg</th>\n",
       "      <th>conserve_9_16sDeepSeg</th>\n",
       "      <th>conserve_2_Vxtractor</th>\n",
       "      <th>conserve_3_Vxtractor</th>\n",
       "      <th>conserve_4_Vxtractor</th>\n",
       "      <th>conserve_5_Vxtractor</th>\n",
       "      <th>conserve_6_Vxtractor</th>\n",
       "      <th>conserve_7_Vxtractor</th>\n",
       "      <th>conserve_8_Vxtractor</th>\n",
       "      <th>conserve_9_Vxtractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CP000302.1607157.1608689</td>\n",
       "      <td>[95, 111]</td>\n",
       "      <td>[312, 355]</td>\n",
       "      <td>[496, 524]</td>\n",
       "      <td>[674, 801]</td>\n",
       "      <td>[902, 976]</td>\n",
       "      <td>[1037, 1105]</td>\n",
       "      <td>[1169, 1229]</td>\n",
       "      <td>[1379, 1399]</td>\n",
       "      <td>[91, 172]</td>\n",
       "      <td>[233, 408]</td>\n",
       "      <td>[470, 580]</td>\n",
       "      <td>[664, 813]</td>\n",
       "      <td>[854, 986]</td>\n",
       "      <td>[1036, 1106]</td>\n",
       "      <td>[1166, 1234]</td>\n",
       "      <td>[1290, 1401]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JQ918082.1.1528</td>\n",
       "      <td>[97, 116]</td>\n",
       "      <td>[334, 377]</td>\n",
       "      <td>[519, 547]</td>\n",
       "      <td>[696, 823]</td>\n",
       "      <td>[925, 999]</td>\n",
       "      <td>[1062, 1130]</td>\n",
       "      <td>[1194, 1254]</td>\n",
       "      <td>[1404, 1424]</td>\n",
       "      <td>[96, 178]</td>\n",
       "      <td>[255, 430]</td>\n",
       "      <td>[493, 603]</td>\n",
       "      <td>[686, 835]</td>\n",
       "      <td>[877, 1009]</td>\n",
       "      <td>[1061, 1131]</td>\n",
       "      <td>[1191, 1259]</td>\n",
       "      <td>[1315, 1426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CP018447.5249193.5250746</td>\n",
       "      <td>[106, 122]</td>\n",
       "      <td>[323, 366]</td>\n",
       "      <td>[507, 535]</td>\n",
       "      <td>[685, 812]</td>\n",
       "      <td>[913, 987]</td>\n",
       "      <td>[1048, 1117]</td>\n",
       "      <td>[1179, 1239]</td>\n",
       "      <td>[1389, 1409]</td>\n",
       "      <td>[102, 183]</td>\n",
       "      <td>[244, 419]</td>\n",
       "      <td>[481, 591]</td>\n",
       "      <td>[675, 824]</td>\n",
       "      <td>[865, 997]</td>\n",
       "      <td>[1047, 1117]</td>\n",
       "      <td>[1176, 1244]</td>\n",
       "      <td>[1300, 1411]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JN536939.1.1442</td>\n",
       "      <td>[56, 74]</td>\n",
       "      <td>[279, 322]</td>\n",
       "      <td>[458, 486]</td>\n",
       "      <td>[638, 763]</td>\n",
       "      <td>[861, 935]</td>\n",
       "      <td>[995, 1063]</td>\n",
       "      <td>[1127, 1181]</td>\n",
       "      <td>[1337, 1357]</td>\n",
       "      <td>[55, 137]</td>\n",
       "      <td>[200, 375]</td>\n",
       "      <td>[432, 542]</td>\n",
       "      <td>[626, 775]</td>\n",
       "      <td>[813, 945]</td>\n",
       "      <td>[994, 1064]</td>\n",
       "      <td>[1124, 1192]</td>\n",
       "      <td>[1247, 1359]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EU461376.1.1386</td>\n",
       "      <td>[104, 123]</td>\n",
       "      <td>[327, 370]</td>\n",
       "      <td>[486, 514]</td>\n",
       "      <td>[664, 791]</td>\n",
       "      <td>[892, 966]</td>\n",
       "      <td>[1030, 1099]</td>\n",
       "      <td>[1155, 1209]</td>\n",
       "      <td>[1368, 1385]</td>\n",
       "      <td>[103, 185]</td>\n",
       "      <td>[248, 423]</td>\n",
       "      <td>[460, 570]</td>\n",
       "      <td>[654, 803]</td>\n",
       "      <td>[844, 976]</td>\n",
       "      <td>[1029, 1099]</td>\n",
       "      <td>[1152, 1220]</td>\n",
       "      <td>[1276, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23884</th>\n",
       "      <td>EU772318.1.1374</td>\n",
       "      <td>[88, 107]</td>\n",
       "      <td>[312, 355]</td>\n",
       "      <td>[474, 502]</td>\n",
       "      <td>[652, 779]</td>\n",
       "      <td>[880, 954]</td>\n",
       "      <td>[1018, 1086]</td>\n",
       "      <td>[1143, 1197]</td>\n",
       "      <td>[1353, 1373]</td>\n",
       "      <td>[87, 169]</td>\n",
       "      <td>[233, 408]</td>\n",
       "      <td>[448, 558]</td>\n",
       "      <td>[642, 791]</td>\n",
       "      <td>[832, 964]</td>\n",
       "      <td>[1017, 1087]</td>\n",
       "      <td>[1140, 1208]</td>\n",
       "      <td>[1264, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23885</th>\n",
       "      <td>AY786080.1.1478</td>\n",
       "      <td>[81, 93]</td>\n",
       "      <td>[288, 323]</td>\n",
       "      <td>[439, 467]</td>\n",
       "      <td>[617, 744]</td>\n",
       "      <td>[844, 918]</td>\n",
       "      <td>[983, 1052]</td>\n",
       "      <td>[1114, 1174]</td>\n",
       "      <td>[1323, 1343]</td>\n",
       "      <td>[73, 154]</td>\n",
       "      <td>[201, 376]</td>\n",
       "      <td>[413, 523]</td>\n",
       "      <td>[607, 756]</td>\n",
       "      <td>[796, 928]</td>\n",
       "      <td>[982, 1052]</td>\n",
       "      <td>[1111, 1179]</td>\n",
       "      <td>[1234, 1345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23886</th>\n",
       "      <td>CP000448.417882.419570</td>\n",
       "      <td>[226, 245]</td>\n",
       "      <td>[464, 507]</td>\n",
       "      <td>[649, 677]</td>\n",
       "      <td>[827, 954]</td>\n",
       "      <td>[1056, 1130]</td>\n",
       "      <td>[1191, 1259]</td>\n",
       "      <td>[1322, 1382]</td>\n",
       "      <td>[1532, 1552]</td>\n",
       "      <td>[225, 307]</td>\n",
       "      <td>[385, 560]</td>\n",
       "      <td>[623, 733]</td>\n",
       "      <td>[817, 966]</td>\n",
       "      <td>[1008, 1140]</td>\n",
       "      <td>[1190, 1260]</td>\n",
       "      <td>[1319, 1387]</td>\n",
       "      <td>[1443, 1554]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23887</th>\n",
       "      <td>KJ572262.1.1357</td>\n",
       "      <td>[26, 45]</td>\n",
       "      <td>[249, 286]</td>\n",
       "      <td>[427, 455]</td>\n",
       "      <td>[605, 732]</td>\n",
       "      <td>[826, 904]</td>\n",
       "      <td>[962, 1030]</td>\n",
       "      <td>[1094, 1154]</td>\n",
       "      <td>[1306, 1325]</td>\n",
       "      <td>[-1, 107]</td>\n",
       "      <td>[170, 344]</td>\n",
       "      <td>[401, 511]</td>\n",
       "      <td>[595, 744]</td>\n",
       "      <td>[782, 914]</td>\n",
       "      <td>[961, 1031]</td>\n",
       "      <td>[1091, 1159]</td>\n",
       "      <td>[1215, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23888</th>\n",
       "      <td>MG004155.1.1430</td>\n",
       "      <td>[52, 68]</td>\n",
       "      <td>[279, 316]</td>\n",
       "      <td>[463, 491]</td>\n",
       "      <td>[641, 768]</td>\n",
       "      <td>[868, 942]</td>\n",
       "      <td>[1005, 1074]</td>\n",
       "      <td>[1135, 1195]</td>\n",
       "      <td>[1345, 1365]</td>\n",
       "      <td>[48, 130]</td>\n",
       "      <td>[200, 375]</td>\n",
       "      <td>[437, 547]</td>\n",
       "      <td>[631, 780]</td>\n",
       "      <td>[820, 952]</td>\n",
       "      <td>[1004, 1074]</td>\n",
       "      <td>[1132, 1200]</td>\n",
       "      <td>[1256, 1367]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23889 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id conserve_2_16sDeepSeg conserve_3_16sDeepSeg  \\\n",
       "0      CP000302.1607157.1608689             [95, 111]            [312, 355]   \n",
       "1               JQ918082.1.1528             [97, 116]            [334, 377]   \n",
       "2      CP018447.5249193.5250746            [106, 122]            [323, 366]   \n",
       "3               JN536939.1.1442              [56, 74]            [279, 322]   \n",
       "4               EU461376.1.1386            [104, 123]            [327, 370]   \n",
       "...                         ...                   ...                   ...   \n",
       "23884           EU772318.1.1374             [88, 107]            [312, 355]   \n",
       "23885           AY786080.1.1478              [81, 93]            [288, 323]   \n",
       "23886    CP000448.417882.419570            [226, 245]            [464, 507]   \n",
       "23887           KJ572262.1.1357              [26, 45]            [249, 286]   \n",
       "23888           MG004155.1.1430              [52, 68]            [279, 316]   \n",
       "\n",
       "      conserve_4_16sDeepSeg conserve_5_16sDeepSeg conserve_6_16sDeepSeg  \\\n",
       "0                [496, 524]            [674, 801]            [902, 976]   \n",
       "1                [519, 547]            [696, 823]            [925, 999]   \n",
       "2                [507, 535]            [685, 812]            [913, 987]   \n",
       "3                [458, 486]            [638, 763]            [861, 935]   \n",
       "4                [486, 514]            [664, 791]            [892, 966]   \n",
       "...                     ...                   ...                   ...   \n",
       "23884            [474, 502]            [652, 779]            [880, 954]   \n",
       "23885            [439, 467]            [617, 744]            [844, 918]   \n",
       "23886            [649, 677]            [827, 954]          [1056, 1130]   \n",
       "23887            [427, 455]            [605, 732]            [826, 904]   \n",
       "23888            [463, 491]            [641, 768]            [868, 942]   \n",
       "\n",
       "      conserve_7_16sDeepSeg conserve_8_16sDeepSeg conserve_9_16sDeepSeg  \\\n",
       "0              [1037, 1105]          [1169, 1229]          [1379, 1399]   \n",
       "1              [1062, 1130]          [1194, 1254]          [1404, 1424]   \n",
       "2              [1048, 1117]          [1179, 1239]          [1389, 1409]   \n",
       "3               [995, 1063]          [1127, 1181]          [1337, 1357]   \n",
       "4              [1030, 1099]          [1155, 1209]          [1368, 1385]   \n",
       "...                     ...                   ...                   ...   \n",
       "23884          [1018, 1086]          [1143, 1197]          [1353, 1373]   \n",
       "23885           [983, 1052]          [1114, 1174]          [1323, 1343]   \n",
       "23886          [1191, 1259]          [1322, 1382]          [1532, 1552]   \n",
       "23887           [962, 1030]          [1094, 1154]          [1306, 1325]   \n",
       "23888          [1005, 1074]          [1135, 1195]          [1345, 1365]   \n",
       "\n",
       "      conserve_2_Vxtractor conserve_3_Vxtractor conserve_4_Vxtractor  \\\n",
       "0                [91, 172]           [233, 408]           [470, 580]   \n",
       "1                [96, 178]           [255, 430]           [493, 603]   \n",
       "2               [102, 183]           [244, 419]           [481, 591]   \n",
       "3                [55, 137]           [200, 375]           [432, 542]   \n",
       "4               [103, 185]           [248, 423]           [460, 570]   \n",
       "...                    ...                  ...                  ...   \n",
       "23884            [87, 169]           [233, 408]           [448, 558]   \n",
       "23885            [73, 154]           [201, 376]           [413, 523]   \n",
       "23886           [225, 307]           [385, 560]           [623, 733]   \n",
       "23887            [-1, 107]           [170, 344]           [401, 511]   \n",
       "23888            [48, 130]           [200, 375]           [437, 547]   \n",
       "\n",
       "      conserve_5_Vxtractor conserve_6_Vxtractor conserve_7_Vxtractor  \\\n",
       "0               [664, 813]           [854, 986]         [1036, 1106]   \n",
       "1               [686, 835]          [877, 1009]         [1061, 1131]   \n",
       "2               [675, 824]           [865, 997]         [1047, 1117]   \n",
       "3               [626, 775]           [813, 945]          [994, 1064]   \n",
       "4               [654, 803]           [844, 976]         [1029, 1099]   \n",
       "...                    ...                  ...                  ...   \n",
       "23884           [642, 791]           [832, 964]         [1017, 1087]   \n",
       "23885           [607, 756]           [796, 928]          [982, 1052]   \n",
       "23886           [817, 966]         [1008, 1140]         [1190, 1260]   \n",
       "23887           [595, 744]           [782, 914]          [961, 1031]   \n",
       "23888           [631, 780]           [820, 952]         [1004, 1074]   \n",
       "\n",
       "      conserve_8_Vxtractor conserve_9_Vxtractor  \n",
       "0             [1166, 1234]         [1290, 1401]  \n",
       "1             [1191, 1259]         [1315, 1426]  \n",
       "2             [1176, 1244]         [1300, 1411]  \n",
       "3             [1124, 1192]         [1247, 1359]  \n",
       "4             [1152, 1220]           [1276, -1]  \n",
       "...                    ...                  ...  \n",
       "23884         [1140, 1208]           [1264, -1]  \n",
       "23885         [1111, 1179]         [1234, 1345]  \n",
       "23886         [1319, 1387]         [1443, 1554]  \n",
       "23887         [1091, 1159]           [1215, -1]  \n",
       "23888         [1132, 1200]         [1256, 1367]  \n",
       "\n",
       "[23889 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此处基本确定Vxtractor覆盖了我们的区域，所以可以不用管这个\n",
    "model_preds = test_one_fold_intersection()\n",
    "for conserve_i in [2, 3, 4, 5, 6, 9]:\n",
    "    model_preds[f'conserve_{conserve_i}_intersec'] = model_preds.apply(lambda x: check_intersection(x[f'conserve_{conserve_i}_Vxtractor'], x[f'conserve_{conserve_i}_16sDeepSeg']), axis = 1)\n",
    "    print(f'Conserve {conserve_i}: ', model_preds.loc[model_preds[f'conserve_{conserve_i}_intersec'] >= 0, f'conserve_{conserve_i}_intersec'].mean())\n",
    "model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:00<00:00, 18.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# 20240428: 首先是把两个模型各自保守区的保守性比较做一个excel表\n",
    "def generate_conserve_fa(pred_df, fasta_file_to, conserve_region = 'conserve_2'):\n",
    "    sequences = []\n",
    "    for _, row in pred_df.iterrows():\n",
    "        seq_id = row['id']\n",
    "        sequence = row['16s_rna']\n",
    "        conserve_start, conserve_end = row[conserve_region]\n",
    "        conserve_sequence = sequence[conserve_start-1:conserve_end]\n",
    "        seq_record = SeqRecord(Seq(conserve_sequence), id=seq_id, description='')\n",
    "        if (conserve_start != -1) and (conserve_end != -1):\n",
    "            if len(conserve_sequence) < 10:\n",
    "                continue # 太短了不能用\n",
    "            sequences.append(seq_record)\n",
    "        else:\n",
    "            # 目前不处理标注为-1的情况，直接看看能标出来的解是否效果更好\n",
    "            pass\n",
    "\n",
    "    SeqIO.write(sequences, fasta_file_to, \"fasta\")\n",
    "    # run muscle to multiple-alignment\n",
    "    cline = MuscleCommandline(\n",
    "        input=fasta_file_to,\n",
    "        out=fasta_file_to.replace(\".fa\", \"_afterMuscle.fa\"),\n",
    "    )\n",
    "    return cline\n",
    "    \n",
    "def test_one_fold_v2(folds_root = crossVal_data_dir, fold_num = 0,):\n",
    "    # 20240425：重新计算一下两个模型的metrics\n",
    "    fold_dir = os.path.join(folds_root, f'Fold_{fold_num}')\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    fold_dir_conserved = os.path.join(fold_dir, f'Conserved')\n",
    "    os.makedirs(fold_dir_conserved, exist_ok=True)\n",
    "    goldenLabel_df = pd.read_csv(f'{folds_root}/testingSet_{fold_num}Fold.csv').iloc[:, :3] # 只需要这里提供的seqs序列\n",
    "    \n",
    "    with open(f\"{fold_dir_conserved}/run_muscle.bash\", \"w\") as f:\n",
    "        for model_checked_nm in ['16sDeepSeg', 'Vxtractor']:\n",
    "            # check 16sDeepSeg & Vxtractor model\n",
    "            if model_checked_nm == 'Vxtractor':\n",
    "                pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv', skiprows=range(2), index_col=False)\n",
    "                cols_obj = ['Sequence'] + [f'V{i+1}' for i in range(9)]\n",
    "                pred_df = pd.DataFrame(pred_df, columns=cols_obj)\n",
    "                pred_df.rename({'Sequence': 'id'}, axis=1, inplace=True)\n",
    "                pred_df['id'] = pred_df['id'].apply(lambda x: x.strip(\"'\"))\n",
    "                for i in range(9):\n",
    "                    pred_df.rename({f'V{i+1}': f'v{i+1}'}, axis=1, inplace=True)\n",
    "                    pred_df[f'v{i+1}'] = pred_df[f'v{i+1}'].apply(lambda x: [int(i.split(' HMM=')[0]) for i in x.strip(\"'\").split('-')] if (('notfound' not in x) and ('wrongorder' not in x)) else [-1, -1])\n",
    "            else:\n",
    "                pred_df = pd.read_csv(f'{folds_root}/Fold_{fold_num}/testing_set_segmentation_{model_checked_nm}.csv')\n",
    "                for i in range(9):\n",
    "                    pred_df[f'v{i+1}'] = pred_df[f'v{i+1}'].apply(lambda x: eval(x))\n",
    "            \n",
    "            # 获取conserved区域信息\n",
    "            pred_df = pd.merge(pred_df, goldenLabel_df, on = 'id', how='left', )\n",
    "            for conserve_i in range(8):\n",
    "                pred_df[f'conserve_{conserve_i+2}'] = pred_df.apply(lambda x: [x[f'v{conserve_i+1}'][-1], x[f'v{conserve_i+2}'][0]] , axis = 1)\n",
    "                cline = generate_conserve_fa(pred_df, f'{fold_dir_conserved}/{model_checked_nm}_conserve_{conserve_i+2}.fa', conserve_region = f'conserve_{conserve_i+2}')\n",
    "                \n",
    "                cline = str(cline).replace(\"-in\", \"-align\").replace(\"-out\", \"-output\")\n",
    "                cline = cline.replace(\"-align\", \"-super5\") + \" -threads 32\"\n",
    "                f.write(str(cline) + ' &\\n')\n",
    "        f.write('wait')\n",
    "    \n",
    "    os.system(\"bash \" + f\"{fold_dir_conserved}/run_muscle.bash\")\n",
    "    return\n",
    "\n",
    "# 跑完了MSA，计算保守性结果的funcs\n",
    "degenerate_base_table = {\n",
    "    \"A\": [\"A\"],\n",
    "    \"T\": [\"T\"],\n",
    "    \"G\": [\"G\"],\n",
    "    \"C\": [\"C\"],\n",
    "    \"Y\": [\"C\", \"T\"],\n",
    "    \"R\": [\"A\", \"G\"],\n",
    "    \"W\": [\"A\", \"T\"],\n",
    "    \"S\": [\"G\", \"C\"],\n",
    "    \"K\": [\"T\", \"G\"],\n",
    "    \"M\": [\"C\", \"A\"],\n",
    "    \"N\": [\"A\", \"T\", \"G\", \"C\"],\n",
    "    \"H\": [\"A\", \"C\", \"T\"],\n",
    "    \"V\": [\"A\", \"C\", \"G\"],\n",
    "    \"B\": [\"C\", \"G\", \"T\"],\n",
    "    \"D\": [\"A\", \"G\", \"T\"],\n",
    "}\n",
    "\n",
    "def parse_MSA(\n",
    "    mas_fna, \n",
    "    deletion_cutoff=0.9,  # 超过多少频率的-会被认为是MSA的不准确导致\n",
    "    ):\n",
    "    if not os.path.exists(mas_fna):\n",
    "        return None, 'Error', 'Error'\n",
    "    parsed_file = mas_fna.replace('afterMuscle.fa', 'parsedMSA.csv')\n",
    "    recs = SeqIO.parse(mas_fna, \"fasta\")\n",
    "    total_seq_num = len(list(recs))\n",
    "    if not os.path.exists(parsed_file):\n",
    "        seqs_np = []\n",
    "        for rec in recs:\n",
    "            seq_now = str(rec.seq)\n",
    "            # 把其他dege的base换成一个普通base\n",
    "            for dege_base, subs_nor_base in degenerate_base_table.items():\n",
    "                subs_nor_base = subs_nor_base[\n",
    "                    0\n",
    "                ]  # 1219：这部分因为使用的是SILVA中的参考序列，这种dege base比例本身不高，参考师兄们的说法就处理成第一个base就行\n",
    "                seq_now = seq_now.replace(dege_base, subs_nor_base)\n",
    "            seqs_np.append(list(seq_now))\n",
    "        seqs_np = np.array(seqs_np)\n",
    "        \n",
    "        # 统计每个位点的atgc分布情况\n",
    "        total_seq_num = seqs_np.shape[0]  # 总共有多少条序列参与了引物设计\n",
    "        pos_conserved = [{\"A\": -1, \"T\": -1, \"G\": -1, \"C\": -1, \"-\": -1}]\n",
    "        for i in range(seqs_np.shape[1]):\n",
    "            seqs_this_pos = list(seqs_np[:, i])\n",
    "            atgc_cnt = Counter(seqs_this_pos)\n",
    "            pos_conserved.append(dict(atgc_cnt))\n",
    "        pos_conserved = pd.DataFrame(pos_conserved)\n",
    "        pos_conserved.fillna(0.0, inplace=True)\n",
    "        pos_conserved = pos_conserved.iloc[1:, :] # 去掉第一行的-1\n",
    "        pos_conserved.reset_index(inplace=True, drop=True)\n",
    "        pos_conserved = pos_conserved.div(total_seq_num, axis = 1)\n",
    "    else:\n",
    "        pos_conserved = pd.read_csv(parsed_file)\n",
    "    \n",
    "    # 注意最开始得到的是按照deletion<0.9进行处理的，因此这里如果新设定deletion_cutoff必须比0.9更小\n",
    "    pos_conserved = pos_conserved.loc[pos_conserved['-'] < deletion_cutoff, :].reset_index(drop=True) # 太多的deletion位点就去掉\n",
    "    \n",
    "    entro_list = [entropy(pos_i, base = np.e) for pos_i in pos_conserved.values]\n",
    "    entro_mean = np.mean(entro_list)\n",
    "    conser_lens = len(entro_list)\n",
    "    return pos_conserved, entro_mean, conser_lens, total_seq_num\n",
    "\n",
    "def test_one_fold_entropyRes_v2(folds_root = crossVal_data_dir, fold_num = 0,):\n",
    "    fold_dir = os.path.join(folds_root, f'Fold_{fold_num}')\n",
    "    fold_dir_conserved = os.path.join(fold_dir, f'Conserved')\n",
    "    entro_df = []\n",
    "    for model_checked_nm in ['16sDeepSeg', 'Vxtractor']:\n",
    "        temp_res = {'Fold': fold_num, 'Methods': model_checked_nm}\n",
    "        for conserve_i in range(8):\n",
    "            pos_conserved, entro_mean, conser_lens, total_seq_num = parse_MSA(f'{fold_dir_conserved}/{model_checked_nm}_conserve_{conserve_i + 2}_afterMuscle.fa', deletion_cutoff = deletion_cutoff)\n",
    "            # 第一遍存储过了，现在别存了 pos_conserved.to_csv(f'{fold_dir_conserved}/{model_checked_nm}_conserve_{conserve_i + 2}_parsedMSA.csv', index = False)\n",
    "            temp_res[f'conserve_{conserve_i + 2}_entropy'] = entro_mean\n",
    "            temp_res[f'conserve_{conserve_i + 2}_lens'] = conser_lens\n",
    "            temp_res[f'conserve_{conserve_i + 2}_Nseqs'] = total_seq_num\n",
    "        entro_df.append(temp_res)\n",
    "    return pd.DataFrame(entro_df)\n",
    "\n",
    "for fold_i in tqdm(range(0, 10)):\n",
    "    # test_one_fold_v2(folds_root = crossVal_data_dir, fold_num = fold_i,) # 20240427 -- 这部分结果已经跑完了可以不用跑节省时间\n",
    "    res_fold = test_one_fold_entropyRes_v2(folds_root = crossVal_data_dir, fold_num = fold_i,)\n",
    "    res_fold.to_csv(f'{crossVal_data_dir}/Fold_{fold_i}/Conserved/res_temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 127.09it/s]\n"
     ]
    }
   ],
   "source": [
    "conservatism_df = []\n",
    "for fold_i in tqdm(range(0, 10)):\n",
    "    temp_res = pd.read_csv(f'{crossVal_data_dir}/Fold_{fold_i}/Conserved/res_temp.csv',)\n",
    "    conservatism_df.append(temp_res)\n",
    "conservatism_df = pd.concat(conservatism_df, axis = 0).reset_index(drop =True)\n",
    "\n",
    "conservatism_stat_df = []\n",
    "for conserve_i in range(8):\n",
    "    temp_res = {}\n",
    "    temp_res[f'conserved_region'] = f'C{conserve_i + 2}'\n",
    "    vx_cons, redseg_cons = list(conservatism_df.loc[conservatism_df['Methods'] == 'Vxtractor', f'conserve_{conserve_i+2}_entropy']), list(conservatism_df.loc[conservatism_df['Methods'] == '16sDeepSeg', f'conserve_{conserve_i+2}_entropy'])\n",
    "    vx_lens, redseg_lens = list(conservatism_df.loc[conservatism_df['Methods'] == 'Vxtractor', f'conserve_{conserve_i+2}_lens']), list(conservatism_df.loc[conservatism_df['Methods'] == '16sDeepSeg', f'conserve_{conserve_i+2}_lens'])\n",
    "    _, pvalue = wilcoxon(vx_cons, redseg_cons)\n",
    "    \n",
    "    temp_res[f'RedSeg_entropy'] = f'{np.mean(redseg_cons):.4f}±{np.std(redseg_cons):.4f} ({np.mean(redseg_lens):.2f}±{np.std(redseg_lens):.2f})'\n",
    "    temp_res[f'Vxtractor_entropy'] = f'{np.mean(vx_cons):.4f}±{np.std(vx_cons):.4f} ({np.mean(vx_lens):.2f}±{np.std(vx_lens):.2f})'\n",
    "    temp_res[f'Wilcox_paired_pvalue'] = pvalue\n",
    "    conservatism_stat_df.append(temp_res)\n",
    "conservatism_stat_df = pd.DataFrame(conservatism_stat_df)\n",
    "conservatism_stat_df.to_csv(f'{crossVal_data_dir}/Saved_figs/Conserved_entropy_stat.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20240427：对那些RedSeg预测出来比较短的conserve区域进行滑窗看是否属于最前列\n",
    "def calculate_conservatism(vxtractor_file, redseg_file, deletion_cutoff=0.1):\n",
    "    # 读取CSV文件\n",
    "    vxtractor_data = pd.read_csv(vxtractor_file,)\n",
    "    redseg_data = pd.read_csv(redseg_file,)\n",
    "    vxtractor_data = vxtractor_data[vxtractor_data['-'] < deletion_cutoff].reset_index(drop=True)\n",
    "    redseg_data = redseg_data[redseg_data['-'] < deletion_cutoff].reset_index(drop=True)\n",
    "    \n",
    "    # 确定窗口宽度为redseg文件的行数\n",
    "    window_width = redseg_data.shape[0]\n",
    "\n",
    "    # 计算滑窗的次数\n",
    "    num_windows = vxtractor_data.shape[0] - window_width + 1\n",
    "    conservatisms = []\n",
    "\n",
    "    # 对于每个滑窗进行计算\n",
    "    for i in range(num_windows):\n",
    "        # 提取滑窗范围内的vxtractor数据\n",
    "        window_data = vxtractor_data.iloc[i:i+window_width, :]\n",
    "        window_conservatism = entropy(window_data.values.T, base=np.e).mean()\n",
    "        conservatisms.append(window_conservatism)\n",
    "    # 计算redseg数据的保守性\n",
    "    redseg_conservatism = entropy(redseg_data.values.T, base=np.e).mean()\n",
    "    \n",
    "    # 判断redseg_conservatism是否在前列\n",
    "    rank = sum([x < redseg_conservatism for x in conservatisms]) + 1\n",
    "    conserved_than_ratio = sum([x >= redseg_conservatism for x in conservatisms]) / len(conservatisms)\n",
    "    \n",
    "    return conservatisms, [redseg_conservatism], rank, conserved_than_ratio\n",
    "\n",
    "def stat_dist(vxtractor_conservatisms, redseg_conservatisms, ax = None, title = '', redseg_ranks = None, conserved_than_ratio = None):\n",
    "    # 进行秩和检验\n",
    "    _, p_value = mannwhitneyu(vxtractor_conservatisms, redseg_conservatisms)\n",
    "    \n",
    "    # 绘制保守性的直方图\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    # 使用 np.histogram 计算每个 bin 的分布\n",
    "    hist, bin_edges = np.histogram(vxtractor_conservatisms, bins=30, density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    ax.bar(bin_centers, hist, width=(bin_edges[1] - bin_edges[0]), alpha=0.6, edgecolor='black')\n",
    "    ax.axvline(np.mean(redseg_conservatisms), color='r', linestyle='--', label='RedSeg', linewidth = 2)\n",
    "    smoothed_hist = gaussian_filter1d(hist, sigma=1)\n",
    "    ax.plot(bin_centers, smoothed_hist, color='black', linestyle='-', )\n",
    "    \n",
    "    ax.set_xlabel('Entropy of sliding windows of V-xtractor')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Frequency of CRs between ' + title)\n",
    "    text_x = np.mean(redseg_conservatisms) + (bin_edges[1] - bin_edges[0]) * 8  # 调整标记的位置\n",
    "    y_lim = np.max(hist) * 1.4\n",
    "    ax.set_ylim([0, y_lim])\n",
    "    text_y = y_lim * 0.82  # 调整标记的高度\n",
    "    # ax.text(text_x, text_y, f'Mean Rank: {np.mean(redseg_ranks):.2f}', ha='center')\n",
    "    ax.text(text_x, text_y, f'{100 - np.mean(conserved_than_ratio) * 100:.2f}±{np.std(conserved_than_ratio) * 100:.2f}%', ha='center')\n",
    "    \n",
    "    return p_value, np.mean(redseg_ranks), np.mean(conserved_than_ratio)\n",
    "    \n",
    "# 创建子图\n",
    "plt.rcParams['font.size'] = 9\n",
    "fig, axs = plt.subplots(2, 3, figsize = (8.27, 5.5))\n",
    "stat_res_df = []\n",
    "fig_i = 0\n",
    "for conserve_i in [2, 3, 4, 5, 6, 9]:\n",
    "    vxtractor_conservatisms, redseg_conservatisms = [], []\n",
    "    redseg_ranks = []\n",
    "    conserved_than_ratio = []\n",
    "    ax = axs[fig_i // 3, fig_i % 3]\n",
    "    fig_i += 1\n",
    "    for fold_i in range(1, 10):\n",
    "        conserve_root = f'/data1/hyzhang/Projects/EcoPrimer_git/DeepEcoPrimer_v2/data/Ten_CrossValidation/Fold_{fold_i}/Conserved'\n",
    "        vxtractor_file = f'{conserve_root}/Vxtractor_conserve_{conserve_i}_parsedMSA.csv'\n",
    "        deepseg_file = f'{conserve_root}/16sDeepSeg_conserve_{conserve_i}_parsedMSA.csv'\n",
    "        vx_cons, redseg_cons, rank, conserved_than_t = calculate_conservatism(vxtractor_file, deepseg_file, deletion_cutoff=deletion_cutoff)\n",
    "        vxtractor_conservatisms += vx_cons\n",
    "        redseg_conservatisms += redseg_cons\n",
    "        redseg_ranks.append(rank)\n",
    "        conserved_than_ratio.append(conserved_than_t)\n",
    "    \n",
    "    p_value, redseg_mean_rank, redseg_top_ratio = stat_dist(vxtractor_conservatisms, redseg_conservatisms, ax = ax, title = f'V{conserve_i-1}-V{conserve_i}', redseg_ranks = redseg_ranks, conserved_than_ratio = conserved_than_ratio)\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.savefig(f'{crossVal_data_dir}/Saved_figs/conservatism_comparison.svg', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f'{crossVal_data_dir}/Saved_figs/conservatism_comparison.pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 重新利用完整数据训练一个RedSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 16sDeepSeg/train.py -c data/Publicated_16sDeepSeg/config_train_publicatedModel.json\n"
     ]
    }
   ],
   "source": [
    "publicated_model_dir = 'data/Publicated_16sDeepSeg'\n",
    "ori_json = '16sDeepSeg/config_train.json'\n",
    "with open(ori_json, \"r\") as file:\n",
    "    ori_json = json.load(file)\n",
    "    \n",
    "def create_config_publicatedModel(ori_json, data_dir = '', save_dir = ''):\n",
    "    json_config = ori_json.copy()\n",
    "    json_config['data_loader']['args']['data_dir'] = data_dir\n",
    "    json_config['trainer']['save_dir'] = save_dir\n",
    "    # 修改log config的位置\n",
    "    json_config['log_config'] = '16sDeepSeg/logger/logger_config.json'\n",
    "    # 修改validation data size\n",
    "    json_config['data_loader']['args']['validation_split'] = 0.02\n",
    "    return json_config\n",
    "\n",
    "train_data = os.path.join(crossVal_data_dir, f\"HVRs_info_merged_complete_clearBac.csv\")\n",
    "save_path = publicated_model_dir\n",
    "config_i = create_config_publicatedModel(ori_json, data_dir = train_data, save_dir = save_path)\n",
    "# 保存相应的json文件\n",
    "out_json_path = os.path.join(publicated_model_dir, f\"config_train_publicatedModel.json\")\n",
    "with open(out_json_path, \"w\") as file:\n",
    "    json.dump(config_i, file, indent=4)\n",
    "# 生成跑模型的命令\n",
    "model_train_py = '16sDeepSeg/train.py'\n",
    "run_cmd = f'python {model_train_py} -c {out_json_path}'\n",
    "\n",
    "print(f'{run_cmd}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>16s_rna</th>\n",
       "      <th>lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CP000302.1607157.1608689</td>\n",
       "      <td>AGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCCTAACACA...</td>\n",
       "      <td>1533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JQ918082.1.1528</td>\n",
       "      <td>AGAGTTTGATCCTGGCTCAGGGCGAACGCTGGCGGCGTGCCTAACA...</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CP018447.5249193.5250746</td>\n",
       "      <td>ACTTAAATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGG...</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JN536939.1.1442</td>\n",
       "      <td>GGCAGGCTTACACATGCAAGTCGAGGGGCAGCAGATCATTTCGGTG...</td>\n",
       "      <td>1442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EU461376.1.1386</td>\n",
       "      <td>AGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCATGCCTAACA...</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>JN038255.1.1495</td>\n",
       "      <td>AGAGTTTGATCATGGCTCAGGACGAACGCTGGCGGCGGGCTTAACA...</td>\n",
       "      <td>1495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>JQ461008.1.1411</td>\n",
       "      <td>GAGTTTGATTCATGGCTCAAGACGAAACGCTGGCGGCGTGCCTAAT...</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>JQ072447.1.1408</td>\n",
       "      <td>CAAGTCGAACGGCAGCACGGGCTTCGGCCTGGTGGCGAGTGGCGGA...</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CR628337.408822.410330</td>\n",
       "      <td>GAAGAGTTTGATCCTGGCTCAGATTGAACGCTGGCGGCATGCTTAA...</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>EU453778.1.1403</td>\n",
       "      <td>AGAGTTTGCCCTTGGGTCAGGATGAACGCTAGCGACAGGCTTAACA...</td>\n",
       "      <td>1403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0   CP000302.1607157.1608689   \n",
       "1            JQ918082.1.1528   \n",
       "2   CP018447.5249193.5250746   \n",
       "3            JN536939.1.1442   \n",
       "4            EU461376.1.1386   \n",
       "..                       ...   \n",
       "95           JN038255.1.1495   \n",
       "96           JQ461008.1.1411   \n",
       "97           JQ072447.1.1408   \n",
       "98    CR628337.408822.410330   \n",
       "99           EU453778.1.1403   \n",
       "\n",
       "                                              16s_rna  lens  \n",
       "0   AGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCCTAACACA...  1533  \n",
       "1   AGAGTTTGATCCTGGCTCAGGGCGAACGCTGGCGGCGTGCCTAACA...  1528  \n",
       "2   ACTTAAATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGG...  1554  \n",
       "3   GGCAGGCTTACACATGCAAGTCGAGGGGCAGCAGATCATTTCGGTG...  1442  \n",
       "4   AGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCATGCCTAACA...  1386  \n",
       "..                                                ...   ...  \n",
       "95  AGAGTTTGATCATGGCTCAGGACGAACGCTGGCGGCGGGCTTAACA...  1495  \n",
       "96  GAGTTTGATTCATGGCTCAAGACGAAACGCTGGCGGCGTGCCTAAT...  1411  \n",
       "97  CAAGTCGAACGGCAGCACGGGCTTCGGCCTGGTGGCGAGTGGCGGA...  1408  \n",
       "98  GAAGAGTTTGATCCTGGCTCAGATTGAACGCTGGCGGCATGCTTAA...  1509  \n",
       "99  AGAGTTTGCCCTTGGGTCAGGATGAACGCTAGCGACAGGCTTAACA...  1403  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 制作一个100条序列的demo文件用于16sDeepSeg的使用说明\n",
    "demo_input = pd.read_csv('data/Ten_CrossValidation/testingSet_0Fold.csv')\n",
    "demo_input = demo_input.iloc[:100, :3]\n",
    "demo_input.to_csv('input/demo_input_16sDeepSeg.csv', index=False)\n",
    "\n",
    "demo_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431575, 3)                   id                                            16s_rna  lens\n",
      "0    AB001445.1.1538  AACTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGC...  1538\n",
      "1  KM209255.204.1909  AGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCCTAACA...  1706\n",
      "2    HL281554.1.1313  GACGAACGCTGGCGGCGTGCTTAACACATGCAAGTCGAACGAGTGG...  1313\n",
      "3    AB002515.1.1332  GCCTAATACATGCAAGTTGACGACAGATGATACGTAGCTTGCTACA...  1332\n",
      "4    AB002523.1.1496  TCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAATACATGCAAGT...  1496\n"
     ]
    }
   ],
   "source": [
    "# 制作一个所有silva序列的文件用于16sDeepSeg进行分割\n",
    "silva_seg_input = pd.read_csv('/data1/hyzhang/Projects/EcoPrimer_git/DeepEcoPrimer_v2/data/SILVA_data/silva_seqs_id_only.csv')\n",
    "silva_seg_input = pd.DataFrame(silva_seg_input, columns=['silva_id', '16s_rna', 'lens'])\n",
    "silva_seg_input.rename(columns={'silva_id': 'id'}, inplace = True)\n",
    "print(silva_seg_input.shape, silva_seg_input.head())\n",
    "\n",
    "silva_seg_input.to_csv('/data1/hyzhang/Projects/EcoPrimer_git/DeepEcoPrimer_v2/data/SILVA_data/silva_input_16sDeepSeg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EcoPrimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
